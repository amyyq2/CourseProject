# -*- coding: utf-8 -*-
"""comments_scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wagmUG00udJl4iagzF6437jG7jJE-INk
"""

!pip install selenium
!apt-get update 
!apt install chromium-chromedriver

import time
from selenium.webdriver import Chrome
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd   
from selenium import webdriver

# creating a dataframe of comments from one youtube video url
# parameter: url to a youtube video
# returns: dataframe of all comments for video
def create_comments_df(youtube_url):
  data=[]
  # driver = webdriver.Chrome('/path/to/chromedriver') 
  chrome_options = webdriver.ChromeOptions()
  chrome_options.add_argument('--headless')
  chrome_options.add_argument('--no-sandbox')
  chrome_options.add_argument('--disable-dev-shm-usage')
  # driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)

  with Chrome('chromedriver',chrome_options=chrome_options) as driver:
      wait = WebDriverWait(driver,15)
      # driver.get("https://www.youtube.com/watch?v=0EVVKs6DQLo") # kiss me more
      # driver.get("https://www.youtube.com/watch?v=YbJOTdZBX1g") # baby shark
      driver.get(youtube_url)

      for item in range(40): # can change this range to get more/less
          wait.until(EC.visibility_of_element_located((By.TAG_NAME, "body"))).send_keys(Keys.END)
          # time.sleep(15)

      comments = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "#content")))
      for comment in comments:
          data.append(comment.text)

  df = pd.DataFrame(data, columns=['comment'])
  return df

df = create_comments_df("https://www.youtube.com/watch?v=0EVVKs6DQLo")

df

"""# Data Cleaning
- Filter out non-english comments
- Filter out EXACT duplicate comments

"""

'''
Removing all non-english comments
'''
!pip install langdetect
from langdetect import detect

def detect_lang(text):
    return detect(text)

"""
Function to clean the dataframe of comments
"""
def clean_df(df):
  # remove first two rows
  df = df.drop([0, 1])
  # print(df)
  # remove any comments that are a language other than English
  try:
    df.drop(df[df['comment'].map(detect_lang) != "en"].index, inplace = True)
  except Exception as err:
    print("error: {0}".format(err))

  # remove duplicate comments
  df.drop_duplicates()
  return df

clean_df(df)

"""# Sentiment Analysis

**Using the Vader Sentiment Analysis package, here is how we will using the output to classify the output sentiment ([from the docs](https://github.com/cjhutto/vaderSentiment)):**

The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.

It is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:

positive sentiment: compound score >= 0.05
neutral sentiment: (compound score > -0.05) and (compound score < 0.05)
negative sentiment: compound score <= -0.05
"""

!pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# function to print sentiments
# of the sentence.
def sentiment_scores(sentence):

 
    # Create a SentimentIntensityAnalyzer object.
    sid_obj = SentimentIntensityAnalyzer()
 
    # polarity_scores method of SentimentIntensityAnalyzer
    # object gives a sentiment dictionary.
    # which contains pos, neg, neu, and compound scores.
    sentiment_dict = sid_obj.polarity_scores(sentence)
     
    # print("Overall sentiment dictionary is : ", sentiment_dict)
    # print("The compound score is: ", sentiment_dict["compound"])
    compound = sentiment_dict["compound"]
    # print("sentence was rated as ", sentiment_dict['neg']*100, "% Negative")
    # print("sentence was rated as ", sentiment_dict['neu']*100, "% Neutral")
    # print("sentence was rated as ", sentiment_dict['pos']*100, "% Positive")
 
    # print("Sentence Overall Rated As", end = " ")

    return compound
 

 

def avg_comment_sentiment(comment_df):

    # convert df to list
    comment_list = comment_df.values.tolist()

    comment_sentiments = [sentiment_scores(comment) for comment in comment_list]

    # print("the comment sentiments are: ", comment_sentiments)

    sentiment_score_total = 0
    count = 0
    for sentiment in comment_sentiments:
        count += 1
        sentiment_score_total = sentiment_score_total + sentiment


    # print("sentiment score total is: ", sentiment_score_total)
    # print("# comments is: ", count)
    overall_sentiment = sentiment_score_total/count

    # print("overall sentiment is: ", overall_sentiment)


    # # decide sentiment as positive, negative and neutral
    # if overall_sentiment >= 0.05 :
    #     print("Positive")
 
    # elif overall_sentiment <= - 0.05 :
    #     print("Negative")
 
    # else :
    #     print("Neutral")
    return  overall_sentiment

test1 = "I hate you"
test2 = "they ATE they ATE they A T E"

sentiment_scores(test1)
sentiment_scores(test2)

# avg_comment_sentiment(df)

"""# Parallelization/Multi-Threading"""

'''
Getting a list of URLs and running our sentiment analysis on each of the videos
'''

def sentiment_analyze_urls(urls):
  # TODO: start comment scraping of all videos at the same time + analyze asap + print out asap/send info asap
  final_sentiments = []
  for url in urls:
    comments_df = create_comments_df(url)
    comments_df = clean_df(comments_df)
    final_sentiments.append(avg_comment_sentiment(comments_df))
  return final_sentiments

# TEST

urls = ["https://www.youtube.com/watch?v=0EVVKs6DQLo", "https://www.youtube.com/watch?v=YbJOTdZBX1g"]
sentiment_analyze_urls(urls)

